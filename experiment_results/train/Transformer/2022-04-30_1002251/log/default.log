2022-04-30 10:02:25,969 - INFO - Config:
2022-04-30 10:02:25,969 - INFO - {
    "L2_regularisation": 0,
    "alpha": 100,
    "base_dir": "models/experiments/final/eICU/LoS/Transformer",
    "batch_norm": "mybatchnorm",
    "batch_size": 64,
    "batch_size_test": 32,
    "batchnorm": "mybatchnorm",
    "d_model": 16,
    "dataset": "eICU",
    "diagnosis_size": 64,
    "disable_cuda": false,
    "exp_name": "Transformer",
    "feedforward_size": 256,
    "intermediate_reporting": false,
    "labs_only": false,
    "last_linear_size": 17,
    "learning_rate": 0.00017,
    "loss": "msle",
    "main_dropout_rate": 0.45,
    "mode": "train",
    "n_epochs": 15,
    "n_heads": 2,
    "n_layers": 6,
    "name": "Transformer",
    "no_diag": false,
    "no_exp": false,
    "no_labs": false,
    "no_mask": false,
    "percentage_data": 100.0,
    "positional_encoding": false,
    "save_results_csv": false,
    "seed": 1836135432,
    "shuffle_train": true,
    "sum_losses": true,
    "task": "LoS",
    "trans_dropout_rate": 0
}
2022-04-30 10:02:31,493 - INFO - Experiment set up.
2022-04-30 10:02:34,924 - INFO - Transformer(
  (relu): ReLU()
  (sigmoid): Sigmoid()
  (hardtanh): Hardtanh(min_val=0.020833333333333332, max_val=100)
  (trans_dropout): Dropout(p=0, inplace=False)
  (main_dropout): Dropout(p=0.45, inplace=False)
  (msle_loss): MSLELoss(
    (squared_error): MSELoss()
  )
  (mse_loss): MSELoss(
    (squared_error): MSELoss()
  )
  (bce_loss): BCELoss()
  (empty_module): EmptyModule()
  (transformer): TransformerEncoder(
    (input_embedding): Conv1d(176, 16, kernel_size=(1,), stride=(1,))
    (pos_encoder): PositionalEncoding()
    (trans_encoder_layer): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
      )
      (linear1): Linear(in_features=16, out_features=256, bias=True)
      (dropout): Dropout(p=0, inplace=False)
      (linear2): Linear(in_features=256, out_features=16, bias=True)
      (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0, inplace=False)
      (dropout2): Dropout(p=0, inplace=False)
    )
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
  )
  (diagnosis_encoder): Linear(in_features=293, out_features=64, bias=True)
  (bn_diagnosis_encoder): MyBatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (point_los): Linear(in_features=145, out_features=17, bias=True)
  (point_mort): Linear(in_features=145, out_features=17, bias=True)
  (bn_point_last_los): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn_point_last_mort): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (point_final_los): Linear(in_features=17, out_features=1, bias=True)
  (point_final_mort): Linear(in_features=17, out_features=1, bias=True)
)
2022-04-30 10:10:12,831 - INFO - Custom bins confusion matrix:
2022-04-30 10:10:12,832 - INFO - [[1003825  824308  149145   56348   27791   15342    8968    5464    9560
     1616]
 [ 447918  511376  133897   61043   32521   18789   11417    6872   12136
     2014]
 [ 215676  309385  103546   52492   29401   17456   10630    6918   12261
     1952]
 [ 117588  196422   79828   42902   25198   15293    9486    6132   11089
     1904]
 [  71173  132674   61454   35282   20878   12918    8081    5137    9800
     1735]
 [  46200   94168   48430   28949   17699   11025    6966    4427    8637
     1454]
 [  31614   70042   39130   23941   14755    9498    5938    3810    7349
     1305]
 [  22080   53528   31762   19995   12563    8105    5050    3318    6155
     1161]
 [  59375  161401  104504   68172   44022   27861   17963   11767   22631
     4125]
 [  31673   92662   68753   47452   31372   20721   13521    8992   18222
     3460]]
2022-04-30 10:10:20,736 - INFO - Epoch: 0 | Train Loss: 118.4268
2022-04-30 10:12:08,754 - INFO - Custom bins confusion matrix:
2022-04-30 10:12:08,755 - INFO - [[200165 183804  33618  16096   8861   4488   1734    567    271      0]
 [ 75208 111164  32123  20402  13487   6919   2585    784    324      0]
 [ 32958  62488  23797  17901  13307   6960   2620    843    341      0]
 [ 16506  36267  17263  14698  12146   6247   2544    734    238      0]
 [  8949  22573  12432  11828  10177   5816   2490    824    309      0]
 [  5674  15261   9491   9504   8723   4995   2228    789    339      0]
 [  3648  10018   7266   8085   7224   4222   1815    697    355      0]
 [  2326   7657   5846   6617   6087   3572   1486    627    349      0]
 [  4725  20393  18707  22498  22049  12896   5787   2685   1280      0]
 [  2475  10417  10177  13575  15110  11663   4878   1927   1459      0]]
2022-04-30 10:12:10,785 - INFO - Epoch: 0 | Validation Loss: 100.3655
2022-04-30 10:19:43,011 - INFO - Custom bins confusion matrix:
2022-04-30 10:19:43,011 - INFO - [[1072789  782599  130455   51612   26306   14447    8450    5259    9060
     1458]
 [ 429687  513352  136944   65939   36002   20918   12403    7680   13254
     1977]
 [ 190338  308151  110116   59381   34543   20653   12645    7936   13913
     2117]
 [  95064  192923   84730   49788   30286   18730   11632    7256   13371
     2086]
 [  53171  127305   65324   40703   25847   16241   10266    6529   11910
     1845]
 [  32836   87955   51002   34032   21515   13833    8735    5674   10669
     1708]
 [  21385   63136   41118   28057   18374   11748    7681    4947    9304
     1594]
 [  14042   46402   33260   23674   15510   10099    6561    4381    8305
     1426]
 [  33825  131969  107374   80175   54918   36577   23919   16072   31017
     5657]
 [  14775   67283   66585   55234   40193   27717   19297   12953   27331
     5418]]
2022-04-30 10:19:50,923 - INFO - Epoch: 1 | Train Loss: 104.6220
2022-04-30 10:21:36,712 - INFO - Custom bins confusion matrix:
2022-04-30 10:21:36,713 - INFO - [[169469 207684  41149  16898   8380   3569   1515    593    347      0]
 [ 57754 122163  39797  22840  12268   5217   1952    675    330      0]
 [ 23711  66675  28701  20747  12637   5513   2047    841    343      0]
 [ 11479  37756  20179  17123  11503   5551   1960    727    365      0]
 [  5924  22783  14596  13663   9739   5616   1975    720    382      0]
 [  3640  15390  10648  10895   8414   5125   1777    675    440      0]
 [  2311   9742   7950   9143   7247   4147   1722    656    412      0]
 [  1486   7072   6430   7423   6094   3462   1564    597    439      0]
 [  3184  18037  19866  24127  22635  12744   5988   2446   1991      2]
 [  1497   8821  10649  14697  15899  10383   5319   2437   1979      0]]
2022-04-30 10:21:38,909 - INFO - Epoch: 1 | Validation Loss: 97.5834
2022-04-30 10:28:55,582 - INFO - Custom bins confusion matrix:
2022-04-30 10:28:55,582 - INFO - [[1040982  821356  128471   50222   24894   13631    8058    4982    8477
     1359]
 [ 399618  541223  140348   67083   36086   20363   12126    7271   12126
     1849]
 [ 171976  321273  113053   61514   35173   20676   12765    7845   13498
     1983]
 [  84244  197528   87509   51980   31104   18876   11747    7445   13283
     2121]
 [  46280  128736   67423   42375   26443   16755   10264    6644   12221
     2026]
 [  28801   86824   52462   35309   22385   14268    9223    5795   11098
     1794]
 [  18381   62489   41384   29293   18906   12317    8010    5229    9674
     1661]
 [  11633   45507   33479   24361   16338   10515    6954    4499    8841
     1544]
 [  27508  126788  106832   82272   57274   37900   25605   16997   33906
     6517]
 [  12022   63721   63828   54992   41206   29613   20269   14003   30562
     6612]]
2022-04-30 10:29:03,498 - INFO - Epoch: 2 | Train Loss: 101.4825
2022-04-30 10:30:48,222 - INFO - Custom bins confusion matrix:
2022-04-30 10:30:48,223 - INFO - [[237718 164340  23561  12055   7072   3138   1166    362    192      0]
 [ 89967 110627  26323  18046  11004   4851   1601    422    155      0]
 [ 38635  65197  20663  17044  11807   5306   1882    560    121      0]
 [ 18977  38817  15747  14465  10684   5544   1669    587    153      0]
 [ 10081  24367  11850  11674   9432   5531   1681    603    179      0]
 [  6268  16707   9046   9446   8166   5053   1601    548    169      0]
 [  4164  10963   6895   7792   6984   4306   1591    462    173      0]
 [  2750   8292   5601   6355   5852   3634   1419    470    194      0]
 [  6000  22892  17298  21601  21957  13295   5164   2000    813      0]
 [  2948  11345   9631  13635  15357  10960   5147   2023    635      0]]
2022-04-30 10:30:50,336 - INFO - Epoch: 2 | Validation Loss: 95.8854
2022-04-30 10:38:13,947 - INFO - Custom bins confusion matrix:
2022-04-30 10:38:13,947 - INFO - [[1032635  827998  131901   50106   24488   13407    7933    4667    8019
     1249]
 [ 387224  547796  146028   68676   36330   20158   11690    7025   11626
     1526]
 [ 163265  322929  118102   63319   36060   20897   12552    7748   13185
     1702]
 [  77887  198706   90344   53694   31673   19358   11813    7523   13078
     1844]
 [  42099  128140   69377   43888   27334   17007   10614    6809   12192
     1758]
 [  25937   85867   53875   35850   23355   14730    9364    6135   11220
     1698]
 [  16025   60949   42600   30077   19675   12712    8305    5422   10074
     1577]
 [   9885   44274   33816   25198   16925   11117    7120    4664    9173
     1545]
 [  23489  120387  106840   84309   58896   40221   27109   18237   35856
     6416]
 [  10467   58040   62711   55349   42951   31041   21718   15174   32601
     6776]]
2022-04-30 10:38:21,953 - INFO - Epoch: 3 | Train Loss: 99.2911
2022-04-30 10:43:06,739 - INFO - Custom bins confusion matrix:
2022-04-30 10:43:06,740 - INFO - [[205149 173180  36303  16798   9549   4745   2170   1072    638      0]
 [ 71819 107586  34655  22076  14436   7099   3272   1338    715      0]
 [ 29412  59752  24996  19041  14545   7555   3473   1584    857      0]
 [ 14122  34122  16956  15425  12518   7620   3498   1464    918      0]
 [  7463  20396  12230  11860  10434   7040   3671   1326    978      0]
 [  4474  13762   9046   9269   8393   6272   3519   1342    927      0]
 [  2891   8898   6341   7316   7380   5340   2914   1325    925      0]
 [  1890   6407   5014   6056   5903   4664   2625   1016    992      0]
 [  3951  16297  15360  19115  21044  17108  10294   4204   3645      2]
 [  1883   7806   8156  11000  13214  12044   9391   4358   3829      0]]
2022-04-30 10:43:08,784 - INFO - Epoch: 3 | Validation Loss: 94.5718
2022-04-30 10:53:27,127 - INFO - Custom bins confusion matrix:
2022-04-30 10:53:27,127 - INFO - [[1032276  830251  131072   49989   24414   13170    7560    4612    7897
     1230]
 [ 381535  551456  147868   69313   36343   20041   11549    7067   11423
     1553]
 [ 159221  323260  119824   65112   36310   21105   12513    7591   13016
     1792]
 [  75320  197345   91564   54817   32549   19888   11951    7594   12896
     1969]
 [  40534  126963   69876   44685   27819   17211   10855    6884   12473
     1864]
 [  24607   84423   53921   36839   24055   15252    9542    6170   11416
     1747]
 [  15351   59265   42666   30752   20481   13220    8223    5567   10248
     1595]
 [   9412   42321   34323   25891   17447   11189    7362    4825    9264
     1635]
 [  22946  113686  105226   85412   61371   41722   28128   18636   37470
     6969]
 [   9295   54732   60328   55003   43187   31938   22889   16320   35002
     8036]]
2022-04-30 10:53:34,900 - INFO - Epoch: 4 | Train Loss: 97.8245
2022-04-30 10:55:20,772 - INFO - Custom bins confusion matrix:
2022-04-30 10:55:20,773 - INFO - [[222153 175892  26560  12656   6970   3408   1229    472    264      0]
 [ 79777 116681  29524  18402  10960   5020   1743    621    268      0]
 [ 32655  67430  23525  17100  11925   5422   2051    755    352      0]
 [ 15653  39650  17042  14328  11040   5740   2040    722    428      0]
 [  8259  24296  12563  11634   9564   5785   2050    809    438      0]
 [  4866  16918   9249   9214   8082   5416   2062    809    388      0]
 [  3216  10861   6907   7580   7133   4551   1902    829    351      0]
 [  2092   7879   5806   6091   5814   4004   1655    814    412      0]
 [  4459  20794  17567  20644  21374  14389   6751   3117   1925      0]
 [  2172   9961   9963  11882  14335  12271   5869   3450   1778      0]]
2022-04-30 10:55:23,032 - INFO - Epoch: 4 | Validation Loss: 93.5986
2022-04-30 11:03:06,894 - INFO - Custom bins confusion matrix:
2022-04-30 11:03:06,894 - INFO - [[1027861  836289  130819   49956   24276   12858    7384    4373    7467
     1172]
 [ 374654  558482  149086   69939   36158   19517   11314    6730   10720
     1537]
 [ 154119  326797  121561   65691   36329   21019   12431    7615   12456
     1774]
 [  72830  197661   93157   55650   32647   19757   11971    7483   12951
     1806]
 [  38714  125830   71289   45807   27979   17610   10850    6937   12322
     1843]
 [  23250   83896   54288   37739   24101   15183    9782    6327   11556
     1861]
 [  14466   58499   42427   31254   20837   13466    8575    5672   10517
     1664]
 [   8953   41497   34177   25966   17855   11627    7613    4851    9501
     1629]
 [  21515  109958  105214   86545   61862   42360   28456   19517   38542
     7425]
 [   8650   51518   59379   55683   44097   32864   23317   16618   36150
     8350]]
2022-04-30 11:03:14,562 - INFO - Epoch: 5 | Train Loss: 96.6879
2022-04-30 11:04:59,549 - INFO - Custom bins confusion matrix:
2022-04-30 11:04:59,550 - INFO - [[234098 166317  25164  11845   6753   3512   1146    499    270      0]
 [ 84152 113669  28633  18201  10623   5175   1715    581    247      0]
 [ 35032  65946  22836  16813  11704   5835   2101    611    337      0]
 [ 16862  39142  16423  14283  10843   5817   2180    710    383      0]
 [  9037  24063  12164  11365   9611   5769   2217    773    399      0]
 [  5368  16875   8871   9062   7963   5442   2271    746    406      0]
 [  3559  10903   6764   7266   6993   4651   2029    809    356      0]
 [  2322   7978   5634   5692   5794   4095   1849    752    451      0]
 [  4838  21381  17247  19429  20617  15125   7297   3341   1739      6]
 [  2389  10439   9473  11222  14176  12618   6251   3272   1841      0]]
2022-04-30 11:05:01,615 - INFO - Epoch: 5 | Validation Loss: 92.9937
2022-04-30 11:12:26,670 - INFO - Custom bins confusion matrix:
2022-04-30 11:12:26,671 - INFO - [[1029437  833977  132631   49832   23592   12695    7237    4426    7500
     1108]
 [ 371500  558495  152355   70018   36102   19712   11078    6663   10742
     1451]
 [ 152349  324898  123866   66220   36683   21191   12555    7553   12699
     1724]
 [  71156  196202   94425   56461   33358   19814   12012    7599   12974
     1852]
 [  37516  124516   72078   46379   28671   17696   10993    7014   12326
     1964]
 [  22451   82312   55069   37721   24710   15728    9892    6408   11669
     1975]
 [  14062   56896   43256   31579   21047   13703    8804    5643   10577
     1785]
 [   8524   40492   33998   26196   18013   11960    7875    5248    9716
     1676]
 [  20217  106846  103848   87785   63748   43522   29402   20001   39154
     7263]
 [   8308   48087   58527   56532   44875   33323   23911   17323   37174
     8768]]
2022-04-30 11:12:34,495 - INFO - Epoch: 6 | Train Loss: 95.6676
2022-04-30 11:14:20,500 - INFO - Custom bins confusion matrix:
2022-04-30 11:14:20,501 - INFO - [[232701 167497  27965  11833   5720   2473    926    371    118      0]
 [ 83591 115061  32157  18217   8717   3446   1198    488    121      0]
 [ 34198  67153  25947  17856   9783   3990   1560    511    217      0]
 [ 16600  39332  19003  15457   9608   4149   1668    569    257      0]
 [  8726  24080  14062  12909   8719   4272   1789    582    259      0]
 [  5133  16864  10650   9972   7630   4086   1795    587    287      0]
 [  3426  10867   7929   8241   6670   3640   1704    609    244      0]
 [  2192   7870   6504   6606   5724   3249   1489    650    283      0]
 [  4694  20816  19633  22861  20330  12631   5843   2932   1272      8]
 [  2433   9778  11284  13506  15004  10212   5481   2586   1397      0]]
2022-04-30 11:14:22,710 - INFO - Epoch: 6 | Validation Loss: 92.5085
2022-04-30 11:21:49,120 - INFO - Custom bins confusion matrix:
2022-04-30 11:21:49,121 - INFO - [[1027185  835822  132837   50305   23997   12738    7164    4228    7007
     1098]
 [ 367736  560963  154064   71010   35963   19426   10729    6495   10239
     1425]
 [ 149647  325793  125462   67591   36933   20778   12149    7520   12191
     1652]
 [  69082  196722   95228   56986   33614   20049   12071    7418   12835
     1818]
 [  36982  123381   72580   47029   29026   18053   11060    7004   12121
     1872]
 [  22179   81262   55334   38383   25035   15897    9890    6409   11668
     1864]
 [  13598   56138   43181   32332   21575   13629    8857    5678   10477
     1858]
 [   8336   39612   33871   26586   18555   12010    7876    5104    9901
     1794]
 [  19810  102756  104462   88577   64277   43994   30090   19968   39939
     7890]
 [   8275   45217   57718   55852   46014   33946   24325   17293   38938
     9250]]
2022-04-30 11:21:56,892 - INFO - Epoch: 7 | Train Loss: 94.9539
2022-04-30 11:23:41,348 - INFO - Custom bins confusion matrix:
2022-04-30 11:23:41,349 - INFO - [[207686 173521  37859  15435   7377   4386   1833    759    748      0]
 [ 69366 113256  38062  20835  11044   5823   2645   1026    939      0]
 [ 27335  63095  28140  18958  11772   6339   3051   1429   1093      3]
 [ 13066  35685  19212  15585  10658   6540   3176   1501   1217      3]
 [  6720  21106  13615  12630   9038   6177   3138   1709   1255     10]
 [  3869  14502   9987   9595   7415   5378   3276   1696   1270     16]
 [  2430   9132   7364   7648   6395   4636   2926   1449   1334     16]
 [  1603   6365   5850   6061   5388   4019   2576   1401   1280     24]
 [  3203  15334  18000  18842  19186  15568  10002   5531   5295     59]
 [  1474   7274   8890  10901  11750  11812   9095   4625   5860      0]]
2022-04-30 11:23:43,418 - INFO - Epoch: 7 | Validation Loss: 91.4893
2022-04-30 11:31:36,403 - INFO - Custom bins confusion matrix:
2022-04-30 11:31:36,404 - INFO - [[1022878  840141  133513   50177   23862   12696    7023    4160    6943
     1013]
 [ 363005  564247  155395   71516   35924   19401   10809    6392    9982
     1368]
 [ 147555  326534  126381   68034   37215   20659   12146    7400   12216
     1609]
 [  68300  196068   95914   57635   33745   19874   12157    7612   12821
     1754]
 [  36425  122891   72595   47444   29513   18056   11121    6969   12402
     1785]
 [  21502   80817   55348   39090   25082   15898   10120    6513   11773
     1892]
 [  13386   55242   43368   32526   21641   13799    8923    5871   10755
     1929]
 [   8017   38670   34510   27079   18465   12114    7817    5240    9874
     1955]
 [  19084   99757  103319   88441   64769   44773   30508   20929   41810
     8436]
 [   7620   42970   55662   56295   46625   35035   25129   18092   40024
     9376]]
2022-04-30 11:31:44,398 - INFO - Epoch: 8 | Train Loss: 94.1908
2022-04-30 11:33:28,300 - INFO - Custom bins confusion matrix:
2022-04-30 11:33:28,301 - INFO - [[226889 170019  29754  12388   6180   2716   1042    493    123      0]
 [ 78828 116278  33386  19237   9245   3860   1304    678    180      0]
 [ 31674  66693  26669  18558  10267   4627   1676    759    292      0]
 [ 15154  38970  18915  15864   9750   5098   1738    822    332      0]
 [  7932  23690  13949  12780   8901   4837   2085    841    383      0]
 [  4690  16423  10275   9889   7799   4590   2156    767    415      0]
 [  3042  10571   7758   7986   6758   4064   1940    774    430      7]
 [  2067   7489   6377   6410   5634   3544   1756    809    464     17]
 [  4247  19412  19147  21327  20949  13102   7345   3252   2219     20]
 [  2003   9567   9971  12275  14637  10908   6549   3042   2729      0]]
2022-04-30 11:33:30,381 - INFO - Epoch: 8 | Validation Loss: 91.3603
2022-04-30 11:41:09,448 - INFO - Custom bins confusion matrix:
2022-04-30 11:41:09,448 - INFO - [[1029099  833779  133743   50204   23925   12536    7052    4194    6858
     1025]
 [ 366372  560170  156279   71524   35966   19200   10696    6419   10182
     1293]
 [ 147506  325017  127060   68650   36942   21173   12305    7269   12176
     1648]
 [  68533  193808   96403   58390   34178   20086   12001    7590   12972
     1859]
 [  35896  121533   73179   48419   29305   18116   11147    7161   12424
     1941]
 [  21173   79151   55849   39356   25477   16066   10173    6577   12097
     1992]
 [  12925   54067   43393   32602   22224   14210    9149    5773   11086
     1867]
 [   7966   37652   33677   27366   19127   12347    8189    5194   10273
     1816]
 [  18560   97423  101900   88723   65668   45622   30846   21064   42987
     8609]
 [   7481   41401   55585   55678   46455   34317   25499   18330   41317
    10715]]
2022-04-30 11:41:17,282 - INFO - Epoch: 9 | Train Loss: 93.5758
2022-04-30 11:43:03,525 - INFO - Custom bins confusion matrix:
2022-04-30 11:43:03,525 - INFO - [[222531 172321  30916  13612   6498   2482    949    231     64      0]
 [ 76282 115964  34565  21166  10013   3361   1227    336     82      0]
 [ 30644  65919  27144  20204  11234   4054   1470    371    175      0]
 [ 14586  38512  18924  17165  10801   4477   1456    492    230      0]
 [  7684  23318  13793  13911   9625   4760   1520    526    261      0]
 [  4453  16060  10251  10967   8188   4671   1684    454    276      0]
 [  2895  10370   7692   8728   7167   4021   1684    544    229      0]
 [  1903   7371   6135   7237   5963   3655   1480    554    269      0]
 [  3940  18555  19298  23121  21994  14289   6171   2483   1163      6]
 [  1920   9014   9668  13349  16194  11607   5877   2570   1482      0]]
2022-04-30 11:43:05,674 - INFO - Epoch: 9 | Validation Loss: 91.1142
2022-04-30 11:50:29,501 - INFO - Custom bins confusion matrix:
2022-04-30 11:50:29,502 - INFO - [[1025526  838627  133888   50242   23478   12210    7063    3931    6485
      938]
 [ 364299  562152  156651   71715   36089   19066   10799    6193    9888
     1260]
 [ 147112  324977  127584   68973   37344   20876   12239    7346   11764
     1618]
 [  67947  195072   95956   58367   34240   20256   12158    7430   12667
     1802]
 [  35912  121486   72973   48110   29836   18503   11079    7012   12416
     1873]
 [  21284   79200   55198   39777   25628   16375   10273    6481   11829
     1970]
 [  13195   53685   43029   32987   22324   14086    8979    6033   11107
     1991]
 [   8037   37216   34064   27152   19029   12771    8161    5324   10069
     1894]
 [  18529   95493  100618   89890   67286   46045   31500   21115   42370
     8836]
 [   7107   40020   53704   56180   46402   35481   25484   18421   42455
    11552]]
2022-04-30 11:50:37,327 - INFO - Epoch: 10 | Train Loss: 93.1675
2022-04-30 11:52:22,911 - INFO - Custom bins confusion matrix:
2022-04-30 11:52:22,912 - INFO - [[208981 191611  27380  11658   5757   2635   1079    385    118      0]
 [ 69541 128404  31932  18134   9018   3879   1375    515    198      0]
 [ 27298  73137  25842  17688   9933   4598   1720    666    333      0]
 [ 12942  42600  18421  15091   9595   5027   1846    739    382      0]
 [  6750  25811  13747  12082   8900   4741   2173    784    410      0]
 [  3805  17893  10131   9531   7712   4577   2143    785    419      8]
 [  2386  11707   7754   7604   6589   4092   1966    788    431     13]
 [  1648   8199   6381   6101   5568   3532   1837    804    474     23]
 [  3366  21080  19408  20288  20380  13367   7470   3346   2261     54]
 [  1687   9874  10208  11928  14235  11140   6511   3104   2994      0]]
2022-04-30 11:52:24,957 - INFO - Epoch: 10 | Validation Loss: 91.2469
2022-04-30 11:59:45,793 - INFO - Custom bins confusion matrix:
2022-04-30 11:59:45,794 - INFO - [[1023056  839997  134885   50344   23629   12249    6807    3869    6616
      957]
 [ 361523  563981  158382   71746   35867   19025   10572    6062    9635
     1297]
 [ 145499  324418  129651   69209   37336   21030   12007    7223   11734
     1690]
 [  66999  193803   97501   59142   34503   20288   11858    7460   12609
     1771]
 [  34902  120755   73579   48754   30012   18299   11380    7186   12413
     1907]
 [  20692   78508   55525   40045   25876   16254   10425    6824   11919
     1915]
 [  12721   53317   43148   33155   22248   14509    9276    6007   11283
     1760]
 [   7657   37263   34147   27365   19078   12519    8231    5323   10307
     1851]
 [  18315   93363  101181   89476   66430   45999   31602   21831   44260
     9369]
 [   7146   38659   52007   54909   46686   36066   26595   19100   43874
    11786]]
2022-04-30 11:59:53,643 - INFO - Epoch: 11 | Train Loss: 92.6619
2022-04-30 12:01:38,336 - INFO - Custom bins confusion matrix:
2022-04-30 12:01:38,337 - INFO - [[240301 160453  25951  11215   5893   3410   1381    709    291      0]
 [ 84845 113168  29988  17325   9479   4931   1953    847    460      0]
 [ 34518  65705  24267  16474  10534   5551   2434   1042    690      0]
 [ 16529  38849  17068  13872   9867   5807   2750   1097    804      0]
 [  8682  23788  12613  11097   8716   5532   2797   1302    871      0]
 [  4910  16660   9364   8673   7277   5167   2795   1274    873     11]
 [  3182  10819   7083   6845   6299   4520   2423   1211    932     16]
 [  2078   7784   5649   5572   5185   3923   2251   1181    920     24]
 [  4407  19563  17690  18017  18379  14400   9136   5153   4216     59]
 [  2176   9230   9063  10008  11818  12124   7846   3957   5459      0]]
2022-04-30 12:01:40,510 - INFO - Epoch: 11 | Validation Loss: 89.9160
2022-04-30 12:09:32,086 - INFO - Custom bins confusion matrix:
2022-04-30 12:09:32,087 - INFO - [[1025930  839049  133663   49832   23343   12365    6829    4033    6510
      896]
 [ 362723  564513  156518   71349   35928   19199   10714    6286    9630
     1234]
 [ 145709  324908  128119   69237   37639   21247   12156    7299   11940
     1502]
 [  66848  194219   96383   59005   34530   20378   12376    7660   12623
     1809]
 [  35560  120409   73128   48277   30248   18312   11360    7131   12788
     1911]
 [  20873   78376   55598   39742   26107   16419   10365    6604   11943
     1908]
 [  12685   53359   43166   32855   22213   14477    9249    6149   11163
     2024]
 [   7818   36687   33977   27382   19221   12575    8154    5420   10420
     2007]
 [  17677   92429  100653   90133   67051   46703   31672   21716   44291
     9253]
 [   6882   36527   52018   55402   46644   36170   26635   19563   45066
    11894]]
2022-04-30 12:09:39,871 - INFO - Epoch: 12 | Train Loss: 92.3321
2022-04-30 12:11:24,420 - INFO - Custom bins confusion matrix:
2022-04-30 12:11:24,421 - INFO - [[233592 166429  29079  11612   5132   2395    977    321     67      0]
 [ 81862 115049  33577  18005   8942   3638   1285    485    153      0]
 [ 33204  66474  27193  17523   9695   4546   1599    679    302      0]
 [ 15712  39350  19418  15114   9356   4699   1885    726    383      0]
 [  8212  24071  14572  12043   8606   4592   2029    788    485      0]
 [  4708  16823  10606   9888   7287   4398   1997    794    502      1]
 [  3078  11007   7933   8029   6180   3989   1725    922    458      9]
 [  2086   7807   6503   6336   5314   3476   1678    867    478     22]
 [  4337  19948  20032  21622  19287  12282   7705   3585   2190     32]
 [  2078   9585  10575  12485  14010  10133   6475   3267   3073      0]]
2022-04-30 12:11:26,470 - INFO - Epoch: 12 | Validation Loss: 90.3218
2022-04-30 12:19:02,141 - INFO - Custom bins confusion matrix:
2022-04-30 12:19:02,142 - INFO - [[1026943  835194  136535   50283   23555   12105    6648    3985    6243
      852]
 [ 363125  560871  159717   72464   35641   19012   10399    6127    9618
     1064]
 [ 145054  323030  129751   70227   37797   21040   12175    7204   11915
     1494]
 [  66348  191936   98252   59520   34612   20653   12436    7487   12854
     1729]
 [  34893  118544   74297   49380   30343   18700   11330    7075   12665
     1917]
 [  20477   77034   55611   40744   26286   16448   10371    6774   12335
     1911]
 [  12670   51935   43277   33356   22438   14796    9339    6105   11530
     1961]
 [   7673   35750   33994   27623   19184   12634    8421    5492   10923
     2023]
 [  17470   89735  100235   88989   68156   47361   32428   22152   45449
     9790]
 [   6550   36151   50412   55233   47333   36449   27068   19730   45750
    12152]]
2022-04-30 12:19:10,043 - INFO - Epoch: 13 | Train Loss: 91.8562
2022-04-30 12:20:55,428 - INFO - Custom bins confusion matrix:
2022-04-30 12:20:55,429 - INFO - [[233227 164973  27705  11251   5991   3406   1644    885    522      0]
 [ 80871 116995  31458  16145   8883   4725   2073   1026    820      0]
 [ 32745  67430  25145  15932   9691   5247   2594   1406   1025      0]
 [ 15539  39437  17904  13607   9267   5355   2851   1527   1156      0]
 [  7998  24118  13018  11268   8106   5062   2917   1560   1351      0]
 [  4462  16628   9838   8681   6840   4591   2942   1591   1418     13]
 [  2897  10771   7223   6861   5945   4155   2404   1513   1541     20]
 [  1923   7493   5818   5810   4709   3652   2295   1449   1394     24]
 [  3890  18727  18577  17625  17285  13712   9012   5739   6369     84]
 [  1930   9009   9302  10079  10990  10177   7931   5042   7218      3]]
2022-04-30 12:20:57,485 - INFO - Epoch: 13 | Validation Loss: 89.6731
2022-04-30 12:28:23,934 - INFO - Custom bins confusion matrix:
2022-04-30 12:28:23,935 - INFO - [[1028732  836676  133380   50620   23039   12211    6695    3960    6198
      935]
 [ 362414  564376  157541   71890   35567   18942   10642    6131    9426
     1147]
 [ 146290  323407  129198   69597   37344   21174   12081    7290   11877
     1566]
 [  66912  192236   97919   59488   34641   20446   12089    7487   12900
     1862]
 [  35127  119244   73742   48608   30457   18710   11387    7303   12715
     1959]
 [  20493   77360   55908   40372   26131   16535   10402    6825   12008
     1997]
 [  12485   52214   43343   33460   22833   14695    9369    5930   11139
     1956]
 [   7567   35989   33669   27826   19380   12771    8473    5482   10548
     2036]
 [  17740   88287   99447   89578   68452   47301   32936   22456   45877
     9625]
 [   6692   34970   49826   53460   46600   36461   26910   20182   47740
    13454]]
2022-04-30 12:28:31,836 - INFO - Epoch: 14 | Train Loss: 91.5199
2022-04-30 12:30:16,857 - INFO - Custom bins confusion matrix:
2022-04-30 12:30:16,858 - INFO - [[203894 189719  30051  13295   6547   3570   1476    739    313      0]
 [ 65687 124631  33075  19788  10831   5414   2200    927    443      0]
 [ 25732  69263  25237  18581  11824   6051   2660   1197    670      0]
 [ 12273  39773  17324  15014  11006   6055   3109   1308    781      0]
 [  6279  24017  12626  11637   9619   5824   3034   1400    959      3]
 [  3565  16608   8922   9215   7997   5255   3040   1428    967      7]
 [  2261  10621   6852   7055   6831   4702   2598   1328   1064     18]
 [  1556   7400   5436   5918   5535   3937   2413   1302   1046     24]
 [  3243  17915  16822  18909  18673  15383   9652   5474   4908     41]
 [  1697   8608   8089  10004  12566  11833   8692   4396   5796      0]]
2022-04-30 12:30:19,103 - INFO - Epoch: 14 | Validation Loss: 89.9363
2022-04-30 12:30:19,105 - INFO - Experiment ended. Checkpoints stored =)
